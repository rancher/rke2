#!/bin/bash

all_services=(
    calico-node
    coredns
    etcd
    kube-apiserver
    kube-controller-manager
    kube-flannel
    kube-proxy
    kube-scheduler
    metrics-server
    rke2-ingress-nginx-controller
    rke2-snapshot-controller
    rke2-snapshot-validation-webhook
)

export NUM_SERVERS=1
export NUM_AGENTS=1
export WAIT_SERVICES="${all_services[@]}"
export SERVER_ARGS="${SERVER_ARGS} --enable-servicelb"

start-test() {
    #docker exec $(cat $TEST_DIR/servers/1/metadata/name) check-config || true
    use-nodelocal-DNSCache
    use-servicelb
    verify-valid-versions $(cat $TEST_DIR/servers/1/metadata/name)
    verify-airgap-images $(cat $TEST_DIR/{servers,agents}/*/metadata/name)
    verify-snapshot-controller
}
export -f start-test

# -- check for changes to the airgap image list
# -- this currently only checks imags for the default CNI and charts
verify-airgap-images() {
    local expected="$TEST_DIR/logs/images-expected.txt"
    local actual="$TEST_DIR/logs/images-actual.txt"
    docker exec $(cat $TEST_DIR/servers/1/metadata/name) cat /images.txt | sort -u >$expected

    for name in $@; do
        docker exec $name crictl images -o json \
            | jq -r '.images[].repoTags[0] | select(. != null)'
    done | sort -u >$actual

    if ! diff $expected $actual; then
        echo '[ERROR] Failed airgap image check'
        return 1
    fi
    return 0
}
export -f verify-airgap-images


# -- Enable the nodelocal DNSCache so that its images are used
use-nodelocal-DNSCache() {
    local DNS_nodeCache_manifest='scripts/airgap/dnsNodeCache-test.yaml'
    kubectl apply -f $DNS_nodeCache_manifest
    wait-for-services node-cache
}
export -f use-nodelocal-DNSCache

# -- Create a loadbalancer service so that the klipper-lb image is used
use-servicelb() {
    local loadbalancer_manifest='scripts/airgap/loadbalancer-test.yaml'
    kubectl apply -f $loadbalancer_manifest
    wait-for-services lb-tcp-8080
}
export -f use-servicelb

# -- Verify that the snapshot controller and validation webhook work,
#    by installing the host-path CSI driver and waiting for a snapshot to be taken.
verify-snapshot-controller() {
  for MANIFEST in $(cat tests/e2e/resource_files/csi-driver-host-path.txt); do
    kubectl create -f ${MANIFEST}
  done
  kubectl wait volumesnapshot new-snapshot-demo --for=jsonpath='{.status.readyToUse}'=true --timeout=2m
}
export -f verify-snapshot-controller


# --- create a basic cluster and check for valid versions
LABEL=BASICS run-test
